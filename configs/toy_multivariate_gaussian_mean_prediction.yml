data_params:
  dataset_name: multivariate_mean_prediction
  root_path: data/toy_data
  generate_dataloaders: False
  train_batch_size: 50
  test_batch_size: 50
  regression: True
  specific_dataset_params: {
    n_clients: 3,
    n_modes: 3,
    n_objects_per_mode_train: 1000,
  }

model_params:
  composition_model_regime: "separate_params"

  shared_model_name: MultivariateCovariance
  shared_model_params: { shared_embedding_size: 0 }

  prior_model_name: MLPEnergyModel
  prior_model_params: { }
  shared_prior_model: False

  personal_model_name: MultivariateMu_personal
  personal_model_params: { }

  backbone_model_name: IdenticalBackbone
  backbone_model_params: { backbone_embedding_size: 2 } # Effectively, it is just input data shape

train_params:
  exp_folder: ./experiment_logs/
  seeds: [ 41, 42, 43 ]

  loss_fn_name: "multivariate_gaussian_ll"
  use_sgld: True

  outer_iters: 100

  inner_iters: 5
  inner_burn_in: 0

  n_personal_models: 3
  clients_sample_size: 1

  device: "cpu"
  prior: gaussian

eval_params:
  metrics: [ multivariate_mse ]


optimization:
  # Personal model optimizer and scheduler parameters
  personal_optimizer: SGLD
  personal_optimizer_params: {
    lr: 0.0001,
    precondition_decay_rate: 0.95,
  }
  personal_scheduler: CyclicLR
  personal_scheduler_params: {
    base_lr: 0.1,
    max_lr: 0.15,
    step_size_up: 10,
    mode: exp_range,
    gamma: 0.99,
    cycle_momentum: False,
  }

  # Personal model optimizer and scheduler parameters
  personal_backbone_optimizer: Adam
  personal_backbone_optimizer_params: { }
  personal_backbone_scheduler: CyclicLR
  personal_backbone_scheduler_params: {
    base_lr: 0.001,
    max_lr: 0.003,
    step_size_up: 10,
    mode: exp_range,
    gamma: 0.99,
    cycle_momentum: False,
  }

  # Shared model optimizer and scheduler parameters
  shared_optimizer: Adam
  shared_optimizer_params: { }
  shared_scheduler: CyclicLR
  shared_scheduler_params: {
    base_lr: 0.01,
    max_lr: 0.3,
    step_size_up: 10,
    mode: exp_range,
    gamma: 0.99,
    cycle_momentum: False,
  }

  # Prior model optimizer and scheduler parameters
  prior_model_optimizer: Adam
  prior_model_optimizer_params: { }
  prior_model_scheduler: CyclicLR
  prior_model_scheduler_params: {
    base_lr: 0.001,
    max_lr: 0.03,
    step_size_up: 10,
    mode: exp_range,
    gamma: 0.99,
    cycle_momentum: False,
  }